# **Context Based LocalLLM** ğŸš€  
A **local-first** implementation of LLM-based code analysis using **ChromaDB** and **Ollama**. This project allows you to index your codebase and query it efficiently using a locally running LLM model.  

## **Features** âœ¨  
âœ… **Local codebase indexing** using ChromaDB  
âœ… **Query your codebase** using a powerful LLM model  
âœ… **No cloud dependency** â€“ Runs entirely on your machine  
âœ… **Customizable** â€“ Configure the LLM model, API endpoint, and file formats  

---

## **Setup Instructions** âš¡  

### **1ï¸âƒ£ Run ChromaDB Locally**
Start a **ChromaDB instance** using Docker:  
```sh
docker run -p 8000:8000 ghcr.io/chroma-core/chroma:latest
